{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bf7b726",
   "metadata": {},
   "source": [
    "# **PROBLEM SOLVING DESIGN**\n",
    "\n",
    "![Lean StartUp Feedback Loop](../img/project_structure/lean_startup_feedback_loop.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "866053b1",
   "metadata": {},
   "source": [
    "# **BUSINESS CONTEXT**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "654c805f",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## **What is the company?**\n",
    "\n",
    "Hotmart\n",
    "\n",
    "## **What is its business model?**\n",
    "\n",
    "Two-sided marketplace. It is a platform for buying, selling and promoting digital products in which Hotmart connects product creators/disseminators to their customers.\n",
    "\n",
    "## **What is the company stage on the market?**\n",
    "\n",
    "\"Virality\" (Lean Analytics) or \"early majority\" (Innovation Adoption Curve). The company found a pain in the market and validated a product that solves the pain; now is the time to increase the customer base."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5329e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T17:35:13.475385Z",
     "start_time": "2021-11-04T17:35:13.469403Z"
    }
   },
   "source": [
    "# **BUSINESS PROBLEM**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f00d37f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T17:35:17.610201Z",
     "start_time": "2021-11-04T17:35:17.584618Z"
    }
   },
   "source": [
    "## **What is the business problem the company is facing?**\n",
    "\n",
    "The company wants to get insight based on customers' data in order to unveil new product opportunities, especially in terms of product success, customer segmentation, and revenue estimation.\n",
    "\n",
    "## **What is the business solution that this project has to deliver?**\n",
    "\n",
    "A presentation of storytelling insights based on the available data and, possibly, answers to the following questions:\n",
    "- Does Hotmart depend on the biggest producers on the platform? That is, the top-selling producers are responsible for most of the\n",
    "Hotmart billing?\n",
    "- Are there any relevant patterns or trends in the data?\n",
    "- It is possible to segment users based on their characteristics (revenue, product niche, etc.)?\n",
    "- What features most impact the success of a product? that is, the What makes a product sell more?\n",
    "- It is possible to estimate how much revenue Hotmart will generate in the next three months from the last month shown in the dataset?\n",
    "\n",
    "**References:**\n",
    "- Case description\n",
    "- https://hotmart.com/pt-br"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cce72a6a",
   "metadata": {},
   "source": [
    "# **SCOPE AND BUSINESS ASSUMPTIONS**\n",
    "\n",
    "- **...**\n",
    "\n",
    "- **...**\n",
    "\n",
    "\n",
    "REFERENCES:\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f8a758",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:34:43.189698Z",
     "start_time": "2021-11-04T13:34:43.022292Z"
    }
   },
   "source": [
    "# **SOLUTION STRATEGY**\n",
    "\n",
    "![IoT method](../img/project_structure/iot_method.png)*IOT (Input-Output-Taks) is a planning strategy to structure a problem solution and make sure it delivers a solution that solves the initial problem.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6ca3bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T17:39:03.938670Z",
     "start_time": "2021-11-04T17:39:03.934341Z"
    }
   },
   "source": [
    "### INPUT\n",
    "\n",
    "- **Business context**:\n",
    "    - It is a platform for buying, selling and promoting digital products in which Hotmart connects product creators/disseminators to their customers.\n",
    "    - In principle, Hotmart makes money by **taxing**, either the creators or the disseminators, **a percentage of the purchase by the customer**.\n",
    "- **Business problem**:\n",
    "    - The company wants to get **insights** based on customers' data in order to **unveil new product opportunities**, especially in terms of product success, customer segmentation, and revenue estimation.\n",
    "- **Business questions**:\n",
    "    - Does **Hotmart depend** on the **biggest producers** on the platform? That is, the **top-selling producers** are responsible for **most** of the Hotmart **billing**?\n",
    "    - Are there any **relevant patterns or trends** in the data?\n",
    "    - It is possible to **segment users** based on their characteristics (revenue, product niche, etc.)?\n",
    "    - What **features most impact** the success of a **product**? that is, the What makes a **product sell more**?\n",
    "    - It is possible to **estimate** how much **revenue** Hotmart will generate in the **next three months from the last month** shown in the dataset?\n",
    "- **Available data**:\n",
    "    - Data referring to a **sample of purchases made** at Hotmart in 2016. These are more than 1.5 million records of purchases made on our **platform**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8df0c314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T17:38:29.559329Z",
     "start_time": "2021-11-04T17:38:29.544358Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### OUTPUT \n",
    "\n",
    "- A presentation of storytelling insights based on the available data and, possibly, answers to the previous questions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa2284b4",
   "metadata": {},
   "source": [
    "### TASKs\n",
    "\n",
    "- *QUESTION*:\n",
    "    - Does **Hotmart depend** on the **biggest producers** on the platform? That is, the **top-selling producers** are responsible for **most** of the Hotmart **billing**?\n",
    "        - What are the biggest producers on the platform? What is its definition?\n",
    "            - Assuming higher than 95th percentile of volume of product sold.\n",
    "        - What it means to be dependent on some producers?\n",
    "            - Assuming \"Pareto rule\" like: 80% of revenue comes from the 5th top selling producers\n",
    "        - What is the revenue difference from this customers to the remaining one?\n",
    "            - Compare revenues\n",
    "\n",
    "<br >\n",
    "\n",
    "- *QUESTION*:\n",
    "    - Are there any **relevant patterns or trends** in the data?\n",
    "        - Check for features (correlation between features, feature distributions and time-changes trends) that shows patterns in terms of customers/producers groups or revenue impact or scaling impact.\n",
    "\n",
    "<br >\n",
    "\n",
    "- *QUESTION*:\n",
    "    - It is possible to **segment users** based on their characteristics (revenue, product niche, etc.)?\n",
    "        - What is the purpose of segmenting customers?\n",
    "          - Find out what are the best customers and what coould be done to change the behaviour of the not-best ones. \n",
    "          - Revenue from best customer could support scaling efforts.\n",
    "        - Check for features that can cluster customer/producers for better revenue undestanding\n",
    "          - Initially try RFM (Recency-Frequency-Monetary)\n",
    "\n",
    "<br >      \n",
    "\n",
    "- *QUESTION*:\n",
    "    - What **features most impact** the success of a **product**? that is, what makes a **product sell more**?\n",
    "        - Success of a product = number of products sold\n",
    "            - Inspect features with high correlation to the number of product sold\n",
    "            - Inspect feature with high correlation with an increasing trend of products sold\n",
    "            - Check for simple causal inference techniques\n",
    "              - knowing features that best impact the product success, we can use this feature for marketing purpose (scalling effort) and, perhaps, get a better overview about what leads to focus on.\n",
    "\n",
    "<br >\n",
    "\n",
    "- *QUESTION*:\n",
    "    - It is possible to **estimate** how much **revenue** Hotmart will generate in the **next three months from the last month** shown in the dataset?\n",
    "        - Check the revenue time-series to understand how to extrapolate it to the future\n",
    "            - Visual inspection\n",
    "            - Check for trend and seasonality and noise\n",
    "            - Define baseline (dummy = last available date)\n",
    "                - Initially, ARIMA model\n",
    "                - If possible, machine learning models\n",
    "                - Check model error and extrapolate to business impact\n",
    "                  - knowing revenue forecast we can predcit scaling investments and even prepone investments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9825219",
   "metadata": {},
   "source": [
    "# **PRODUCT BUILDING ROADMAP**\n",
    "\n",
    "![CRISP-DS Framework](../img/project_structure/crisp_ds.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa8ba4f7",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfb7b1da",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# **0 - HELPERS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f73c6dcd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 0.1 - Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a213f9e",
   "metadata": {},
   "source": [
    "*Import required libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't cache libraries (especially project library)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d066e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and environment\n",
    "import os\n",
    "from   pathlib import Path\n",
    "\n",
    "# save artifacts\n",
    "import pickle\n",
    "\n",
    "# data extraction\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "# statistics\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# sklearn pipelines\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "set_config(display='diagram')\n",
    "\n",
    "# model tracking\n",
    "import mlflow\n",
    "\n",
    "# clustering\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# time series\n",
    "import pmdarima as pm\n",
    "from prophet import Prophet\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf, plot_predict\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# project library\n",
    "from project_lib.initial_config import initial_settings\n",
    "from project_lib.data_description import (check_dataframe, inspect_dtypes, \n",
    "                                          check_na_unique_dtypes, check_dtype_convertion,\n",
    "                                          summary_statistics, categorical_summary, datetime_summary\n",
    "                                          )\n",
    "from project_lib.data_exploration import (numerical_plot, categorical_plot, datetime_plot,\n",
    "                                          create_cramer_v_dataframe)\n",
    "from project_lib.ab_testing import check_for_bias\n",
    "from project_lib.clustering_analysis import silhouette_inspection_pipelined"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66efdb94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T11:06:42.870317Z",
     "start_time": "2021-11-08T11:06:42.862040Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 0.2 - Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e94d8f7",
   "metadata": {},
   "source": [
    "*Define functions that will be used on the project*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "005d90e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T14:13:12.396952Z",
     "start_time": "2021-12-10T14:13:12.316411Z"
    },
    "hidden": true
   },
   "source": [
    "NOTE: Most functions made for this project are inside the project library. That is, **a package called \"project_lib\" was created to hold all functions that will be needed for this project.**\n",
    "\n",
    "\n",
    "For further details, please check the modules inside \"project_lib\" package [in other words, check .py files inside project_lib folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of function created for this project\n",
    "help(check_dataframe)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31bfa06e",
   "metadata": {},
   "source": [
    "## 0.3 - Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7dc1ec1d",
   "metadata": {},
   "source": [
    "*Define basic configurations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66009c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup of dataframes and plots\n",
    "initial_settings(storytelling=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa085249",
   "metadata": {},
   "source": [
    "## 0.4 - Constants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "059932be",
   "metadata": {},
   "source": [
    "*Define reusuable constants*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the project root path that will be the \"baseline\" for all paths in the notebook\n",
    "PROJECT_ROOT_PATH = Path.cwd().parent\n",
    "PROJECT_ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc48c5-8f19-484d-aa2e-71ca25a41b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # variables to connect to data source\n",
    "# HOST=os.environ[\"HOST\"]\n",
    "# PORT=os.environ[\"PORT\"]\n",
    "# USER=os.environ[\"USER\"]\n",
    "# PASSWORD=os.environ[\"PASSWORD\"]\n",
    "# SCHEMA=os.environ[\"SCHEMA\"]\n",
    "# TABLE=os.environ[\"TABLE\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71c139dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T23:09:51.772110Z",
     "start_time": "2021-11-04T23:09:51.767141Z"
    }
   },
   "source": [
    "# **1 - DATA EXTRACTION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "870fc960",
   "metadata": {},
   "source": [
    "## 1.1 - Entity Relationship Diagram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52b1c61e",
   "metadata": {},
   "source": [
    "*Display Entity-Relationship Diagram to a better data understanding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee358fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not available -> datasets are already merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eddb0851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T11:11:43.415835Z",
     "start_time": "2021-11-08T11:11:43.409069Z"
    }
   },
   "source": [
    "## 1.2 - Data Fields Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ef925d8",
   "metadata": {},
   "source": [
    "*Describe available data in regard to database information*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae37d62c-5cfa-4997-87dc-552340480a3c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Na Hotmart, possuímos três principais personas que integram nosso negócio: os produtores, os afiliados e os compradores.\n",
    "- Produtores são pessoas que criam produtos digitais na Hotmart, como cursos de idiomas, ebooks de receitas culinárias, audiolivros, softwares, dentre muitos outros exemplos.\n",
    "- Afiliados são pessoas que promovem produtos dos produtores em troca de uma comissão na venda, que varia de produto para produto, e de afiliado para afiliado.\n",
    "- Compradores são pessoas que adquirem um ou mais produtos digitais.\n",
    "    \n",
    "Uma venda é feita por um afiliado quando alguém clica em um link de afiliados. Eles geralmente fazem a promoção desses produtos em redes sociais, vídeos, anúncios, etc.\n",
    "\n",
    "Já uma venda é feita por um produtor quando alguém tem acesso direto ao seu produto, sem intermediação do afiliado. Por exemplo, pessoas que seguem o Whindersson Nunes no Youtube e entraram em seu site oficial para adquirir seu produto, ou clicaram no link do produto sem código de afiliação.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a995eff-1a03-4512-91b0-254468c3b809",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Durante sua avaliação, você irá analisar dados referentes a uma amostra de compras feitas na Hotmart em 2016. Tratam-se de mais de 1,5 milhão registros de compras realizadas em nossa plataforma. Abaixo, nós iremos detalhar o que significa cada campo:\n",
    "- **purchase_id**: Identificação da compra na Hotmart;\n",
    "- **product_id**: Identificação do produto na Hotmart;\n",
    "- **affiliate_id**: Identificação do afiliado na Hotmart;\n",
    "- **producer_id**: Identificação do produtor na Hotmart;\n",
    "- **buyer_id**: Identificação do comprador na Hotmart;\n",
    "- **purchase_date**: Data e hora em que a compra foi realizada;\n",
    "- **product_creation_date**: Data e hora em que o produto foi criado na Hotmart;\n",
    "- **product_category**: categoria do produto na Hotmart. Exemplo: e-book, software, curso online, e-tickets, etc.;\n",
    "- **product_niche**: nicho de mercado que o produto faz parte. Exemplo: educação, saúde e bem-estar, sexualidade, etc.;\n",
    "- **purchase_value**: valor da compra. Esse dado, assim como nicho e categoria foi codificado para manter a confidencialidade. O valor apresentado no dataset é o z-score do valor real;\n",
    "- **affiliate_commission_percentual**: percentual de comissão que o afiliado receberá da compra;\n",
    "- **purchase_device**: tipo de dispositivo utilizado no momento da compra, como: Desktop, Mobile, Tablet, ou Outros;\n",
    "- **purchase_origin**: endereço do site do qual a pessoa veio antes da compra. Por exemplo, se uma pessoa veio do Facebook, Youtube, ou até mesmo de outra página no site oficial do produto;\n",
    "- **is_origin_page_social_network**: informa se essa compra veio de uma URL do Facebook, Youtube, Instagram, Pinterest, ou Twitter.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76df06ec-313d-4bf6-b1ed-413e2c43e926",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Algumas regras de negócio:\n",
    "- Quando a compra for feita diretamente pelo produtor, ou seja, quando não houver afiliado intermediando a compra, o campo affiliate_commission_percentual terá valor 0, e o campo affiliate_id será igual ao producer_id;\n",
    "- No campo purchase_origin nós apenas consideramos o host do site. Isso quer dizer que, se uma pessoa veio do site www.meuproduto.com/promocoes, esse campo só irá retornar o valor www.meuproduto.com;\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "425ae4b4",
   "metadata": {},
   "source": [
    "## 1.3 - Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98e6ffec",
   "metadata": {},
   "source": [
    "*Load data from required files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856158a-8b04-4d68-b6ea-c46789fffaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define connection \"endpoint\"\n",
    "# db_connection_str = f'mysql+pymysql://{USER}:{PASSWORD}@{HOST}/{SCHEMA}'\n",
    "# # create an engine to connect to database\n",
    "# db_connection = create_engine(db_connection_str)\n",
    "\n",
    "# # define query to get data\n",
    "# query=f\"\"\"\n",
    "# SELECT *\n",
    "# FROM {TABLE}\n",
    "# \"\"\"\n",
    "\n",
    "# # read all data from database\n",
    "# df_sql = pd.read_sql(sql=query, con=db_connection)\n",
    "# df_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9aaf2e-15e2-4bd0-b305-d1e0b27a5619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save data to parquet so as to not overload database server unnecessarily\n",
    "# df_sql.to_parquet(\n",
    "#     path=os.path.join(PROJECT_ROOT_PATH, \"data\", \"raw_data\", \"customer_data.parquet\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b6096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from local source\n",
    "df_extraction = pd.read_parquet(\n",
    "    path=os.path.join(PROJECT_ROOT_PATH, \"data\", \"raw_data\", \"customer_data.parquet\")\n",
    ")\n",
    "\n",
    "# inspect results\n",
    "df_extraction.sample(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fab65071",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# **2 - DATA DESCRIPTION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4baca2ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T11:33:34.436022Z",
     "start_time": "2021-11-08T11:33:34.433283Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.1 - Restore Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8fff667",
   "metadata": {},
   "source": [
    "*Create a checkpoint of the last dataframe from previous section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798a081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:35:09.995864Z",
     "start_time": "2021-11-22T11:35:09.300194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a restore point of the previous section\n",
    "df_description = df_extraction.copy()\n",
    "\n",
    "# check dataframe for this new section\n",
    "check_dataframe( dataframe=df_description, summary_stats=True, head=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc77fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous section dataframe to save memory\n",
    "lst = [df_extraction]\n",
    "del lst\n",
    "del df_extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f1557c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T11:35:57.560444Z",
     "start_time": "2021-11-08T11:35:57.552443Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.2 - Rename Columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "300574e9",
   "metadata": {},
   "source": [
    "*Search for misleading or error-prone column names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b35f8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:35:10.018834Z",
     "start_time": "2021-11-22T11:35:10.014437Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect column names\n",
    "df_description.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79529779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower the case of venda column\n",
    "df_description = df_description.rename(columns={\"Venda\": \"sell\"})\n",
    "\n",
    "# inspect results\n",
    "df_description.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cbdc1b6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.3 - Check Data Dimensions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee1a1d9f",
   "metadata": {},
   "source": [
    "*Check dataframe dimensions to know if pandas will be enough to handle such data size or we will need Big Data tools like Spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b4e12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:35:10.025219Z",
     "start_time": "2021-11-22T11:35:10.020770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# check number of rows and columns\n",
    "print( f'\\\n",
    "Dataframe has {df_description.shape[0]:,} \\\n",
    "rows and {df_description.shape[1]} columns' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a2f8863",
   "metadata": {},
   "source": [
    "## 2.4 - Data Types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2018dc53",
   "metadata": {},
   "source": [
    "*Check if data types on dataframe makes sense according to database information*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87e28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shape before dtype convertion\n",
    "shape_before = df_description.shape\n",
    "\n",
    "# inspect dataframe types\n",
    "inspect_dtypes(df_description, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3285be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect basic column descriptions\n",
    "check_na_unique_dtypes(df_description);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print report\n",
    "print(\n",
    "    f\"Unique values in colum 'sell': {set(df_description['sell'].tolist())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print report\n",
    "print(\n",
    "    f\"Unique values in colum 'sell': {set(df_description['is_origin_page_social_network'].tolist())}\"\n",
    ")\n",
    "\n",
    "# convert column is_origin_page_social_network to boolean\n",
    "df_description[\"is_origin_page_social_network\"] = df_description[\"is_origin_page_social_network\"].apply( lambda x: True if x == '0,0' else False if x == '1,0' else \"NaN\")\n",
    "\n",
    "# print report\n",
    "print(\n",
    "    f\"Unique values in colum 'sell' after transformation: {set(df_description['is_origin_page_social_network'].tolist())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert df_description.shape == shape_before, \"Data was missed during dtype convertion\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaf6f518",
   "metadata": {},
   "source": [
    "## 2.5 - Data Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9b811b1",
   "metadata": {},
   "source": [
    "*Check if columns make sense in regard to business understanding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as data was already made available to us \n",
    "# and there is no way to validate data source,\n",
    "# no need for data validation right now."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0d5576b",
   "metadata": {},
   "source": [
    "## 2.6 - Check Duplicated Rows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9a70dc0",
   "metadata": {},
   "source": [
    "*Inspect duplicated rows and handle them properly*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df49971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataframe grain\n",
    "grain = [\"purchase_id\"]\n",
    "\n",
    "# check duplicated rows\n",
    "print(\n",
    "    f'{\"*\"*49}\\n\\n'\n",
    "    f'There are {df_description.duplicated(keep=False).sum():,} '\n",
    "    f'duplicated rows [{df_description.duplicated(keep=False).mean()*100:.2f}%] based on all columns. '\n",
    "    f'Duplicated rows are double counted.'\n",
    "    f'\\n\\n{\"*\"*49}\\n\\n'\n",
    "    f'Dataframe granularity: {grain}\\n\\n'\n",
    "    f'There are {df_description.duplicated(subset=grain, keep=False).sum():,} duplicated rows '\n",
    "    f'[{df_description.duplicated(subset=grain, keep=False).mean()*100:.2f}%] based on table granularity. '\n",
    "    f'Duplicated rows are double counted.'\n",
    "    f'\\n\\n{\"*\"*49}'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4627b5a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T11:40:01.154007Z",
     "start_time": "2021-11-08T11:40:01.126048Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.7 - Check Missing Values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d8c1102",
   "metadata": {},
   "source": [
    "*Inspect number and percentage of missing value per column to decide what to do with them*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f5e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  get number of NA, percent of NA, number of unique and column type\n",
    "check_na_unique_dtypes(df_description);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print report\n",
    "print(\n",
    "    f'affiliate_commission_percentual\\n'\n",
    "    f'\\tmax value {df_description[\"affiliate_commission_percentual\"].max(skipna=True)}\\n'\n",
    "    f'\\tmin value {df_description[\"affiliate_commission_percentual\"].min(skipna=True)}'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99df2de1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.8 - Handle Missing Values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efea71d8",
   "metadata": {},
   "source": [
    "*Handle missing value for columns*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14b28ea1",
   "metadata": {},
   "source": [
    "**Business rule**\n",
    "- Quando a compra for feita diretamente pelo produtor, ou seja, quando não houver afiliado intermediando a compra, o campo affiliate_commission_percentual terá valor 0, e o campo affiliate_id será igual ao producer_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ff3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of NaN in affiliate_commission_percentual\n",
    "num_nas = df_description[\"affiliate_commission_percentual\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ebe4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect rows where affiliate_commission_percentual is NaN to validate business rule\n",
    "df_description.loc[\n",
    "    df_description[\"affiliate_commission_percentual\"].isna(),\n",
    "    [\"affiliate_commission_percentual\", \"affiliate_id\", \"producer_id\"]    \n",
    "].sample(5, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc8b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as NaNs in affiliate_commission_percentual don't seem to be due to business rule,\n",
    "# let's fill NaN with -1 (number outside of the scope of min-max range)\n",
    "df_description[\"affiliate_commission_percentual\"] = df_description[\"affiliate_commission_percentual\"].fillna(value=-1)\n",
    "\n",
    "# sanity check\n",
    "assert (df_description[\"affiliate_commission_percentual\"] == -1).sum() == num_nas, \"Misleading fillna operation\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "918bd63b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2.9 - Descriptive Statistics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66576dc1",
   "metadata": {},
   "source": [
    "*Inspect some summary statistics for numerical columns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea18362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into types of features\n",
    "df_number = df_description.select_dtypes(include=[\"number\", \"bool\"])\n",
    "df_date = df_description.select_dtypes(include=[\"datetime\"])\n",
    "df_string = df_description.select_dtypes(include=[\"object\"])\n",
    "\n",
    "# sanity check\n",
    "assert df_number.shape[1] + df_date.shape[1] + df_string.shape[1] == df_description.shape[1], \"\"\"Revise the previous split, something may be wrong!\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90cd3688",
   "metadata": {},
   "source": [
    "### 2.9.1 - Numerical Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21704db6",
   "metadata": {},
   "source": [
    "*Inspect numerical variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary statistics\n",
    "summary_statistics(df_number)\n",
    "\n",
    "# delete previous dataframe to save memory\n",
    "lst = [df_number]\n",
    "del lst\n",
    "del df_number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55c73cb6",
   "metadata": {},
   "source": [
    "According to business rule:\n",
    "- purchase_value: \"valor da compra. Esse dado, assim como nicho e categoria foi codificado para manter a  confidencialidade. O valor apresentado no dataset é o **z-score** do valor real\";\n",
    "  - So it is fine to have negative values!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5f415f3",
   "metadata": {},
   "source": [
    "### 2.9.2 - Categorical Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd102a7d",
   "metadata": {},
   "source": [
    "*Inspect categorical variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad44e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check overview of categorical features\n",
    "categorical_summary(df_string, nunique_threshold=30, unique_name_len_threshold=50)\n",
    "\n",
    "# delete previous dataframe to save memory\n",
    "lst = [df_string]\n",
    "del lst\n",
    "del df_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "429001da",
   "metadata": {},
   "source": [
    "### 2.9.3 - Datetime Variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ff30514",
   "metadata": {},
   "source": [
    "*Inspect datetime variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e847fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check an overview of datetime features\n",
    "datetime_summary(df_date)\n",
    "\n",
    "# delete previous dataframe to save memory\n",
    "lst = [df_date]\n",
    "del lst\n",
    "del df_date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcba07f5",
   "metadata": {},
   "source": [
    "### 2.9.4 - Investigate further:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e66e2046",
   "metadata": {},
   "source": [
    "*Variables to inspect the real meaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e384c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None up to this point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0782f000",
   "metadata": {},
   "source": [
    "# **3 - FEATURE ENGINEERING**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15c2eb92",
   "metadata": {},
   "source": [
    "## 3.1 - Restore Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86ffb7ec",
   "metadata": {},
   "source": [
    "*Create a checkpoint of the last dataframe from previous section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a restore point for the previous section dataframe\n",
    "df_f_eng = df_description.copy()\n",
    "\n",
    "# check dataframe\n",
    "check_dataframe( df_f_eng )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous section dataframe to save memory\n",
    "lst = [df_description]\n",
    "del lst\n",
    "del df_description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fc8ce4a",
   "metadata": {},
   "source": [
    "## 3.2 - Hypothesis Testing List"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1aeb871f",
   "metadata": {},
   "source": [
    "*Define the list of hypotheses that will be validated during Exploratory Data Analysis (EDA)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e96ccd38",
   "metadata": {},
   "source": [
    "**HYPOTHESIS MIND MAP**\n",
    "\n",
    "![Business hypothesis mindmap](../img/project_structure/xxx.jpg)\n",
    "\n",
    "*The above image is the product of a brainstorm that took into consideration many different variables that can impact the main business metric. This mind map is a great help when trying to raise hypotheses that could lead to insights. It is also helpful to guide feature engineering (create new relevant features) and when there is a need to look for more data elsewhere.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "517c742d",
   "metadata": {},
   "source": [
    "> *Taking into consideration hypothesis mind map (at the beginning of this notebook) and the business case questions:*\n",
    "\n",
    "\n",
    "**H1**. Does **Hotmart depend** on the **biggest producers** on the platform? That is, the **top-selling producers** are responsible for **most** of the Hotmart **billing**?\n",
    "\n",
    "**H2**. Are there any **relevant patterns or trends** in the data?\n",
    "\n",
    "**H3**. It is possible to **segment users** based on their characteristics (revenue, product niche, etc.)?\n",
    "\n",
    "**H4**. What **features most impact** the success of a **product**? that is, the What makes a **product sell more**?\n",
    "\n",
    "**H5**. It is possible to **estimate** how much **revenue** Hotmart will generate in the **next three months from the last month** shown in the dataset?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75ba067e",
   "metadata": {},
   "source": [
    "## 3.3 - Feature Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4301622",
   "metadata": {},
   "source": [
    "*Create new features (columns) that can be meaningful for EDA and, especially, machine learning modelling.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dba8423",
   "metadata": {},
   "source": [
    "### product_age_when_purchased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column to indicate what is the age of the product when it was purchased\n",
    "# purchase_date - product_creation_date in months\n",
    "# month = 0 ---> purchased on the month of creation\n",
    "df_f_eng[\"product_age_when_purchased\"] = df_f_eng[\"purchase_date\"].dt.to_period(freq=\"M\") - df_f_eng[\"product_creation_date\"].dt.to_period(freq=\"M\")\n",
    "# extract the month information\n",
    "df_f_eng[\"product_age_when_purchased\"] = df_f_eng[\"product_age_when_purchased\"].apply(lambda x: x.n)\n",
    "\n",
    "# inspect result\n",
    "df_f_eng[[\"product_creation_date\", \"purchase_date\", \"product_age_when_purchased\"]].sample(10, random_state=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f9be036",
   "metadata": {},
   "source": [
    "### binned_affiliate_commission_percentual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check numerical stats for affiliate_commission_percentual and product_age_when_purchased\n",
    "summary_statistics(df_f_eng[[\"affiliate_commission_percentual\", \"product_age_when_purchased\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d404cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defiene bins to discretize affiliate_commission_percentual\n",
    "bins = [-1] + [*range(0, 111, 10)] \n",
    "print(f\"Chosen bins: {bins}\")\n",
    "\n",
    "# create a new column with discretized affiliate_commission_percentual\n",
    "df_f_eng[\"binned_affiliate_commission_percentual\"] = pd.cut(x=df_f_eng[\"affiliate_commission_percentual\"], \n",
    "       bins=bins, right=False, retbins=False, include_lowest=True, ordered=True)\n",
    "\n",
    "# inspect results\n",
    "df_f_eng[[\"affiliate_commission_percentual\", \"binned_affiliate_commission_percentual\"]].sample(10, random_state=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "218a38b5",
   "metadata": {},
   "source": [
    "### binned_product_age_when_purchased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f0c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defiene bins to discretize product_age_when_purchased\n",
    "bins = [*range(0, 110, 10)] \n",
    "print(f\"Chosen bins: {bins}\")\n",
    "\n",
    "# create a new column with discretized product_age_when_purchased\n",
    "df_f_eng[\"binned_product_age_when_purchased\"] = pd.cut(x=df_f_eng[\"product_age_when_purchased\"], \n",
    "       bins=bins, right=False, include_lowest=True,\n",
    "       retbins=False, \n",
    "       ordered=True,\n",
    "       )\n",
    "\n",
    "# inspect results\n",
    "df_f_eng[[\"product_age_when_purchased\", \"binned_product_age_when_purchased\"]].sample(10, random_state=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee43aa73",
   "metadata": {},
   "source": [
    "### purchase_date_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert purchase_date to daily frequency and keep it as datetime format\n",
    "df_f_eng[\"purchase_date_daily\"] = df_f_eng[\"purchase_date\"].dt.strftime('%Y-%m-%d')\n",
    "df_f_eng[\"purchase_date_daily\"] = pd.to_datetime(df_f_eng[\"purchase_date_daily\"], format='%Y-%m-%d')\n",
    "\n",
    "# inspect results\n",
    "display(\n",
    "    df_f_eng[[\"purchase_date\", \"purchase_date_daily\"]].dtypes,\n",
    "    df_f_eng[[\"purchase_date\", \"purchase_date_daily\"]].sample(10, random_state=7)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "755eba7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T17:29:18.464606Z",
     "start_time": "2021-11-08T17:29:18.458947Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# **4 - DATA FILTERING**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "026ae2aa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 4.1 - Restore Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb4c9722",
   "metadata": {},
   "source": [
    "*Create a checkpoint of the last dataframe from previous section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569c1ae8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:35:16.188448Z",
     "start_time": "2021-11-22T11:35:15.658676Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a restore point for the previous section dataframe\n",
    "df_filter = df_f_eng.copy()\n",
    "\n",
    "# check dataframe\n",
    "check_dataframe( df_filter, summary_stats=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous section dataframe to save memory\n",
    "lst = [df_f_eng]\n",
    "del lst\n",
    "del df_f_eng"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "196cddb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T18:08:30.512496Z",
     "start_time": "2021-11-08T18:08:30.507268Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 4.2 Rows Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8253646d",
   "metadata": {},
   "source": [
    "*Remove rows with meaningless (or unimportant) data*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28b456d1",
   "metadata": {},
   "source": [
    "### purchase_value column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a1e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to business rule:\n",
    "# - purchase_value: valor da compra. Esse dado, assim como nicho e categoria foi codificado para manter a  confidencialidade. O valor apresentado no dataset é o **z-score** do valor real;\n",
    "# So it is fine to have negative values! ---> no need to filter rows!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c244bd4",
   "metadata": {},
   "source": [
    "### product_age_when_purchased column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457347a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:35:16.193901Z",
     "start_time": "2021-11-22T11:35:16.190106Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check negative product_age_when_purchased\n",
    "df_filter[df_filter[\"product_age_when_purchased\"] < 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef558e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shape before filtering data\n",
    "shape_before = df_filter.shape\n",
    "\n",
    "# it order to avoid misleading data (product was sold before being created)\n",
    "# we will remove these rows\n",
    "df_filter = df_filter[df_filter[\"product_age_when_purchased\"] >= 0]\n",
    "\n",
    "# sanity check\n",
    "assert (\n",
    "    df_filter.shape[0] == shape_before[0] - 2\n",
    ") & (\n",
    "    df_filter.shape[1] == shape_before[1]\n",
    "), \"Misleading rows filtering!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ca35d9b",
   "metadata": {},
   "source": [
    "## 4.3 - Columns Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9acd7efd",
   "metadata": {},
   "source": [
    "*Remove auxiliary columns or columns that won't be available in the prediction moment*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c088b9e",
   "metadata": {},
   "source": [
    "### sell column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd86022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO\n",
    "# print report\n",
    "print(\n",
    "    f\"Unique values in colum 'sell': {set(df_filter['sell'].tolist())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shape before filtering data\n",
    "shape_before = df_filter.shape\n",
    "\n",
    "# column sell is a constant column ---> remove it\n",
    "df_filter = df_filter.drop(columns=[\"sell\"])\n",
    "\n",
    "# sanity check\n",
    "# sanity check\n",
    "assert (\n",
    "    df_filter.shape[0] == shape_before[0]\n",
    ") & (\n",
    "    df_filter.shape[1] == shape_before[1] - 1\n",
    "), \"Misleading columns filtering!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57f868ce",
   "metadata": {},
   "source": [
    "### purchase origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60564f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect column\n",
    "check_na_unique_dtypes(df_filter[[\"purchase_origin\"]]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8eb3388",
   "metadata": {},
   "source": [
    "*Purchase origin is a categorical column with more than 9.000 unique values. So, it would be quite complex to analyse this feature at this point of the project. Then, we decided to remove this column for this project cycle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e79daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shape before filtering data\n",
    "shape_before = df_filter.shape\n",
    "\n",
    "# remove column\n",
    "df_filter = df_filter.drop(columns=[\"purchase_origin\"])\n",
    "\n",
    "# sanity check\n",
    "# sanity check\n",
    "assert (\n",
    "    df_filter.shape[0] == shape_before[0]\n",
    ") & (\n",
    "    df_filter.shape[1] == shape_before[1] - 1\n",
    "), \"Misleading columns filtering!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36158fe8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# **5 - EXPLORATORY DATA ANALYSIS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e109374b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 5.1 - Restore Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b6245ee",
   "metadata": {},
   "source": [
    "*Create a checkpoint of the last dataframe from previous section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1c6c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:35:18.508075Z",
     "start_time": "2021-11-22T11:35:17.992047Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a restore point for the previous section dataframe\n",
    "df_eda = df_filter.copy()\n",
    "\n",
    "# check dataframe\n",
    "check_dataframe( df_eda )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ae26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous section dataframe to save memory\n",
    "lst = [df_filter]\n",
    "del lst\n",
    "del df_filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ce04da7",
   "metadata": {},
   "source": [
    "## 5.2 - Univariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "251f387b",
   "metadata": {},
   "source": [
    "*Explore variables distributions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into types of features\n",
    "df_eda_num = df_eda.select_dtypes(include=[\"number\", \"bool\"])\n",
    "df_eda_date = df_eda.select_dtypes(include=[\"datetime\"])\n",
    "df_eda_str = df_eda.select_dtypes(include=[\"object\", \"category\"])\n",
    "\n",
    "# sanity check\n",
    "assert df_eda_num.shape[1] + df_eda_date.shape[1] + df_eda_str.shape[1] == df_eda.shape[1], \"\"\"Revise the previous split, something may be wrong!\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2453e220",
   "metadata": {},
   "source": [
    "### 5.2.1 - Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define categorical figure path\n",
    "numerical_fig_path = os.path.join(PROJECT_ROOT_PATH, \"img\", \"data_exploration\", \"numerical_fatures_eda.png\")\n",
    "\n",
    "# # plot numerical columns for base data\n",
    "# numerical_plot(\n",
    "#     dataframe=df_eda_num, \n",
    "#     n_cols=3,\n",
    "#     hist=False,\n",
    "#     save_fig=numerical_fig_path\n",
    "#     )\n",
    "\n",
    "# delete previous dataframe to save memory\n",
    "lst = [df_eda_num]\n",
    "del lst\n",
    "del df_eda_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25650dc5",
   "metadata": {},
   "source": [
    "![Numerical Univariate EDA](../img/data_exploration/numerical_fatures_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc3d072e",
   "metadata": {},
   "source": [
    "### 5.2.2 - Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2107307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define categorical figure path\n",
    "categorical_fig_path = os.path.join(PROJECT_ROOT_PATH, \"img\", \"data_exploration\", \"categorical_fatures_eda.png\")\n",
    "\n",
    "# # plot categorical columns for base data\n",
    "# categorical_plot(\n",
    "#     dataframe=df_eda_str,\n",
    "#     max_num_cat=10,\n",
    "#     n_cols=3,\n",
    "#     trunc_label=20,\n",
    "#     save_fig=categorical_fig_path\n",
    "#     )\n",
    "\n",
    "# delete previous dataframe to save memory\n",
    "lst = [df_eda_str]\n",
    "del lst\n",
    "del df_eda_str"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb18f06f",
   "metadata": {},
   "source": [
    "![Categorical Univariate EDA](../img/data_exploration/categorical_fatures_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e51d0134",
   "metadata": {},
   "source": [
    "### 5.2.3 Datetime Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ea868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datetime figure path\n",
    "datetime_fig_path = os.path.join(PROJECT_ROOT_PATH, \"img\", \"data_exploration\", \"datetime_fatures_eda.png\")\n",
    "\n",
    "# # plot datetime columns for base data\n",
    "# datetime_plot(\n",
    "#     dataframe=df_eda_date,\n",
    "#     n_cols=3,\n",
    "#     save_fig=datetime_fig_path\n",
    "#     )\n",
    "\n",
    "# delete previous dataframe to save memory\n",
    "lst = [df_eda_date]\n",
    "del lst\n",
    "del df_eda_date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7853d86b",
   "metadata": {},
   "source": [
    "![Datetime Univariate EDA](../img/data_exploration/datetime_fatures_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "186dfadf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 5.3 - Bivariate Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45447dfc",
   "metadata": {},
   "source": [
    "*Explore relationship between variables (in pairs)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20845368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:35:39.228869Z",
     "start_time": "2021-11-22T11:35:39.226949Z"
    },
    "hidden": true
   },
   "source": [
    "### 5.3.1 - Numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4910e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pairplot figure path\n",
    "pairplot_fig_path = os.path.join(PROJECT_ROOT_PATH, \"img\", \"data_exploration\", \"pairplot_eda.png\")\n",
    "\n",
    "# # plot pairplot\n",
    "# # don't consider id columns -> make no sense as they are \"random\"\n",
    "# g = sns.pairplot( df_eda[[\n",
    "#     'purchase_value', 'affiliate_commission_percentual',\n",
    "#     'is_origin_page_social_network', 'product_age_when_purchased'\n",
    "#     ]].sample(100_000), diag_kind=\"kde\", corner=False )\n",
    "# # define plot details\n",
    "# for ax in g.axes.flatten():\n",
    "#     # rotate x axis labels\n",
    "#     ax.set_xlabel(ax.get_xlabel(), rotation = 45)\n",
    "#     # rotate y axis labels\n",
    "#     ax.set_ylabel(ax.get_ylabel(), rotation = 45)\n",
    "#     # set y labels alignment\n",
    "#     ax.yaxis.get_label().set_horizontalalignment('right')\n",
    "\n",
    "# # save figure\n",
    "# plt.savefig(pairplot_fig_path, facecolor=\"white\", bbox_inches=\"tight\");        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09514e22",
   "metadata": {},
   "source": [
    "![Pairplot EDA](../img/data_exploration/pairplot_eda.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79118ce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:35:39.939255Z",
     "start_time": "2021-11-22T11:35:39.230647Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define pairplot figure path\n",
    "spearman_corr_fig_path = os.path.join(PROJECT_ROOT_PATH, \"img\", \"data_exploration\", \"spearman_corr_eda.png\")\n",
    "\n",
    "# # calculate pearson correlation coefficient\n",
    "# # don't consider id columns -> make no sense as they are \"random\"\n",
    "# correlation = df_eda_num[[\n",
    "#     'purchase_value', 'affiliate_commission_percentual',\n",
    "#     'is_origin_page_social_network', 'product_age_when_purchased'\n",
    "#     ]].corr( method = 'spearman' )\n",
    "\n",
    "# # display heatmap of correlation on figure\n",
    "# sns.heatmap( correlation, annot = True, fmt=\".2f\", cmap=\"Blues\")\n",
    "# plt.yticks( rotation = 0 )\n",
    "# # save figure\n",
    "# plt.savefig(spearman_corr_fig_path, facecolor=\"white\", bbox_inches=\"tight\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52fbcbfc",
   "metadata": {},
   "source": [
    "![Spearman Corr EDA](../img/data_exploration/spearman_corr_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c916bafd",
   "metadata": {},
   "source": [
    "### 5.3.2 - Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bfc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a dataframe with cramer-v for every row-column pair\n",
    "# cramer_v_corr = create_cramer_v_dataframe( df_eda_str )\n",
    "\n",
    "# # inspect results\n",
    "# cramer_v_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define pairplot figure path\n",
    "# cramer_corr_fig_path = os.path.join(PROJECT_ROOT_PATH, \"img\", \"data_exploration\", \"cramer_corr_eda.png\")\n",
    "\n",
    "# # display heatmap of correlation on figure\n",
    "# sns.heatmap( cramer_v_corr, annot = True, fmt=\".2f\", cmap=\"Blues\")\n",
    "# plt.yticks( rotation = 0 )\n",
    "# # save figure\n",
    "# plt.savefig(cramer_corr_fig_path, facecolor=\"white\", bbox_inches=\"tight\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e88f4b82",
   "metadata": {},
   "source": [
    "![Spearman Corr EDA](../img/data_exploration/cramer_corr_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29a6ec3d",
   "metadata": {},
   "source": [
    "### 5.3.3 - Correlation with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e408ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check available columns\n",
    "df_eda.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d223d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate over columns to check for correlation with target variable\n",
    "# for col in ['purchase_date', 'product_creation_date', \n",
    "#             'product_category', \n",
    "#             'product_niche', \n",
    "#             'affiliate_commission_percentual', 'purchase_device', \n",
    "#             'purchase_origin', 'is_origin_page_social_network',\n",
    "#             'product_age_when_purchased']:\n",
    "#     # inspect correlation\n",
    "#     check_for_bias(\n",
    "#         dataframe=df_eda[[col, 'purchase_value']].sample(100_000),\n",
    "#         treatment=\"purchase_value\",\n",
    "#         showfliers=False,\n",
    "#         figsize=(30, 6),\n",
    "#         num_cols=3,\n",
    "#         saving_path=os.path.join(PROJECT_ROOT_PATH, \"img\", \"data_exploration\", f\"{col}_vs_purchase_value_eda.png\")\n",
    "#     );"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9c4094d",
   "metadata": {},
   "source": [
    "![Feature vs Target EDA](../img/data_exploration/product_category_vs_purchase_value_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23cfc960",
   "metadata": {},
   "source": [
    "![Feature vs Target EDA](../img/data_exploration/product_niche_vs_purchase_value_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3cedf97",
   "metadata": {},
   "source": [
    "![Feature vs Target EDA](../img/data_exploration/purchase_device_vs_purchase_value_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a834899",
   "metadata": {},
   "source": [
    "![Feature vs Target EDA](../img/data_exploration/purchase_origin_vs_purchase_value_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4198f44f",
   "metadata": {},
   "source": [
    "![Feature vs Target EDA](../img/data_exploration/affiliate_commission_percentual_vs_purchase_value_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87332a39",
   "metadata": {},
   "source": [
    "![Feature vs Target EDA](../img/data_exploration/product_age_when_purchased_vs_purchase_value_eda.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "606d451c",
   "metadata": {},
   "source": [
    "## 5.4 - Business Hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0876f828",
   "metadata": {},
   "source": [
    "*Validate all business hypothesis based on available data*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19ed7371",
   "metadata": {},
   "source": [
    "### **H1.** Does **Hotmart depend** on the **biggest producers** on the platform? That is, the **top-selling producers** are responsible for **most** of the Hotmart **billing**?\n",
    "        - What are the biggest producers on the platform? What is its definition?\n",
    "            - Assuming higher than 95th percentile of volume of product sold.\n",
    "        - What it means to be dependent on some producers?\n",
    "            - Assuming \"Pareto rule\" like: 80% of revenue comes from the 5th top selling producers\n",
    "        - What is the revenue difference from this customers to the remaining one?\n",
    "            - Compare revenues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ed314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total purchases per producer\n",
    "df_producer_purchases = df_eda.groupby(\n",
    "    by=\"producer_id\", as_index=False\n",
    "    ).agg({\"purchase_value\": \"sum\"})\n",
    "# inspect results\n",
    "df_producer_purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect numerical distribution\n",
    "numerical_plot(df_producer_purchases[[\"purchase_value\"]], hist=False, figsize=(15, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 95 percentile in terms of total purchases from producers\n",
    "purchase_95_quantile = df_producer_purchases[\"purchase_value\"].quantile(q=0.95)\n",
    "# create a new column to indicate top producers\n",
    "df_producer_purchases[\"top_5_percent_producers\"] = df_producer_purchases[\"purchase_value\"] >= purchase_95_quantile\n",
    "\n",
    "# inspect results\n",
    "df_producer_purchases.sample(10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect boxplot\n",
    "sns.boxplot(\n",
    "    data=df_producer_purchases, \n",
    "    y=\"purchase_value\", x=\"top_5_percent_producers\",\n",
    "    meanline=True, showmeans=True, \n",
    "    meanprops={\"color\": \"black\", \"marker\": \"*\"},\n",
    "    showfliers=False\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b237a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split purchase_value into top percent producers\n",
    "data_one = df_producer_purchases.loc[df_producer_purchases[\"top_5_percent_producers\"], \"purchase_value\"]\n",
    "data_two = df_producer_purchases.loc[~df_producer_purchases[\"top_5_percent_producers\"], \"purchase_value\"]\n",
    "\n",
    "# perform Mann-Whitney U Test\n",
    "# Tests whether the distributions of two independent samples are equal or not.\n",
    "# Assumptions\n",
    "#   Observations in each sample are independent and identically distributed (iid).\n",
    "#   Observations in each sample can be ranked.\n",
    "# Interpretation\n",
    "#   H0: the distributions of both samples are equal.\n",
    "#   H1: the distributions of both samples are not equal.\n",
    "stat, p_value = mannwhitneyu(\n",
    "    x=data_one, y=data_two,\n",
    "    alternative='two-sided', \n",
    "    nan_policy='raise'    \n",
    "    )\n",
    "\n",
    "# print report\n",
    "print(\n",
    "    f\"p-value = {p_value:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_one.mean(), data_two.mean(), df_producer_purchases[\"purchase_value\"].mean(), df_eda[\"purchase_value\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b382a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous section dataframe to save memory\n",
    "lst = [df_producer_purchases]\n",
    "del lst\n",
    "del df_producer_purchases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcbe783d",
   "metadata": {},
   "source": [
    "### **H2.** Are there any **relevant patterns or trends** in the data?\n",
    "        - Check for features (correlation between features, feature distributions and time-changes trends) that shows patterns in terms of customers/producers groups or revenue impact or scaling impact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataframe\n",
    "df_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over columns to plot\n",
    "for feature in [\n",
    "    \"product_category\", \"product_niche\", \"purchase_device\", \"is_origin_page_social_network\",\n",
    "    \"binned_affiliate_commission_percentual\", \"binned_product_age_when_purchased\"\n",
    "    ]:\n",
    "    # group data by feature and calculate meaningful aggregations for the analysis\n",
    "    df_product_category_group = df_eda.groupby(\n",
    "        by=feature, as_index=False).agg(\n",
    "        nunique_buyer_id = (\"buyer_id\", \"nunique\"),\n",
    "        nunique_purchase_id = (\"purchase_id\", \"nunique\"),\n",
    "        sum_purchase_value = (\"purchase_value\", \"sum\"),\n",
    "    )\n",
    "\n",
    "    # plot feature in respect to nunique_buyer_id, nunique_purchase_id and sum_purchase_value\n",
    "    df_product_category_group.plot.bar(\n",
    "        x=feature, subplots=True, figsize=(15, 12),\n",
    "        title=feature.upper(), grid=True\n",
    "        );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a1380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over columns to plot\n",
    "for feature in [\n",
    "    \"product_category\", \"product_niche\", \"purchase_device\", \"is_origin_page_social_network\",\n",
    "    \"binned_affiliate_commission_percentual\", \"binned_product_age_when_purchased\"\n",
    "    ]:\n",
    "\n",
    "    # check if feature is categorical\n",
    "    if feature in (\"binned_affiliate_commission_percentual\", \"binned_product_age_when_purchased\"):\n",
    "        # convert categorical to string\n",
    "        dtype_convert = {feature: \"str\"}\n",
    "    # feature is not categorical\n",
    "    else:\n",
    "        # not need to convert\n",
    "        dtype_convert = {}\n",
    "\n",
    "    # define figure layout\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10,12))\n",
    "\n",
    "    # define figure title\n",
    "    fig.suptitle(feature.upper())\n",
    "\n",
    "    # group data by feature and calculate meaningful aggregations for the analysis\n",
    "    df_product_category_group = df_eda.astype(dtype_convert).groupby(\n",
    "        by=[feature, \"purchase_date_daily\"], as_index=False).agg(\n",
    "        nunique_buyer_id = (\"buyer_id\", \"nunique\"),\n",
    "        nunique_purchase_id = (\"purchase_id\", \"nunique\"),\n",
    "        sum_purchase_value = (\"purchase_value\", \"sum\"),\n",
    "    )\n",
    "\n",
    "    # iterate over target variables for subplots\n",
    "    for idx, target in enumerate([\"nunique_buyer_id\", \"nunique_purchase_id\", \"sum_purchase_value\"]):\n",
    "        # plot a line on subplot\n",
    "        sns.lineplot(\n",
    "            data=df_product_category_group, \n",
    "            x=\"purchase_date_daily\", \n",
    "            y=target, \n",
    "            hue=feature,\n",
    "            ax=axs[idx])\n",
    "        # define legend position\n",
    "        axs[idx].legend(bbox_to_anchor=(1.01, 1))\n",
    "    \n",
    "    # display\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df7048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous section dataframe to save memory\n",
    "lst = [df_product_category_group]\n",
    "del lst\n",
    "del df_product_category_group"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b8d3eb4",
   "metadata": {},
   "source": [
    "### **H3.** It is possible to **segment users** based on their characteristics (revenue, product niche, etc.)?\n",
    "        - What is the purpose of segmenting customers?\n",
    "          - Find out what are the best customers and what coould be done to change the behaviour of the not-best ones. \n",
    "          - Revenue from best customer could support scaling efforts.\n",
    "        - Check for features that can cluster customer/producers for better revenue undestanding\n",
    "          - Initially try RFM (Recency-Frequency-Monetary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2427dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get latest purchase date\n",
    "latest_purchase_date = df_eda[\"purchase_date\"].max()\n",
    "# print report\n",
    "print(\n",
    "    f\"Latest purchase data on dataframe: {latest_purchase_date}\"\n",
    ")\n",
    "\n",
    "# group data by customer and aggregate required column to create RFM score\n",
    "df_rfm = df_eda.groupby(by=\"buyer_id\", as_index=False).agg(\n",
    "    # ======= Recency ======= #\n",
    "    # Create Recency feature -> time interval to the last purchase.\n",
    "    # Once the dataframe is outdated, recency will be calculated \n",
    "    # with respect to the most recent date\n",
    "    latest_customer_purchase = (\"purchase_date\", \"max\"),\n",
    "    # ======= Frequency ======= #\n",
    "    # create Frequency feature -> how many times the customer made a purchase\n",
    "    frequency = (\"purchase_id\", \"nunique\"),\n",
    "    # ======= Monetary =======#\n",
    "    # create Monetary -> total spent by customer\n",
    "    # 🚨 ! remember that purchase_value is on Z-scale -> positive and/or negative values ! 🚨\n",
    "    monetary = (\"purchase_value\", \"sum\"),\n",
    ")\n",
    "\n",
    "# get how many day between customer latest purchase date and data collection\n",
    "df_rfm[\"recency\"] = (latest_purchase_date - df_rfm[\"latest_customer_purchase\"]).dt.days\n",
    "# create a constant column with latest_customer_purchase\n",
    "df_rfm[\"db_latest_purchase_date\"] = latest_purchase_date\n",
    "\n",
    "# get shape before reshaping\n",
    "shape_before=df_rfm.shape\n",
    "\n",
    "# reorder columns\n",
    "df_rfm = df_rfm[[\"buyer_id\", \"db_latest_purchase_date\", \"latest_customer_purchase\",\n",
    "                 \"frequency\", \"monetary\", \"recency\"]]\n",
    "\n",
    "# sanity check\n",
    "assert (df_rfm.shape[0] == shape_before[0]) & (df_rfm.shape[1] == shape_before[1]), \"Error when reordering columns!\"\n",
    "\n",
    "# inspect results\n",
    "df_rfm.sample(10, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dict to store RFM transformation\n",
    "rfm_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca4c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataframe shape before feature engineering\n",
    "shape_before = df_rfm.shape\n",
    "\n",
    "\n",
    "# create R feature -> recency feature in a scale from 1 until 5\n",
    "# calculate values to cut R into bins into discrete intervals.\n",
    "recency_qcut = pd.qcut(x=df_rfm[\"recency\"], q=5, labels=False, retbins=True, \n",
    "                       precision=3, duplicates='raise')\n",
    "# assgin R bins\n",
    "df_rfm['R'] = recency_qcut[0]\n",
    "# save bins limits\n",
    "rfm_transform[\"R\"] = {\"right\": True, \"bins\": recency_qcut[1]}\n",
    "\n",
    "\n",
    "# create M feature -> monetary feature in a scale from 1 until 5\n",
    "# calculate values to cut M into bins into discrete intervals.\n",
    "monetary_qcut = pd.qcut(x=df_rfm[\"monetary\"], q=5, labels=False, retbins=True, \n",
    "                     precision=3, duplicates='raise')\n",
    "# assgin M bins\n",
    "df_rfm['M'] = monetary_qcut[0]\n",
    "# save bins limits\n",
    "rfm_transform[\"M\"] = {\"right\": True, \"bins\": monetary_qcut[1]}\n",
    "\n",
    "\n",
    "# sanity check\n",
    "assert (df_rfm.shape[0] == shape_before[0]) & ((df_rfm.shape[1] == shape_before[1] +2)), \"Misleading feature engineering processing!\"\n",
    "\n",
    "# inspect results\n",
    "display(\n",
    "    rfm_transform, \n",
    "    df_rfm.sample(10, random_state=7)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect frequancy\n",
    "sns.ecdfplot(data=df_rfm, x=\"frequency\", stat=\"proportion\")\n",
    "# plot details\n",
    "plt.ylim(0, 1.05)\n",
    "plt.title(\"ECDF plot for frequency\")\n",
    "plt.xlim(0, 10)\n",
    "plt.xticks([*range(1, 10)])\n",
    "plt.yticks([i/100 for i in range(0, 101, 5)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7247d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define quantiles to inspect\n",
    "q = [0.5, 0.8, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99] # lazy! \n",
    "# iterate over quantiles to inspect\n",
    "for q_ in q:\n",
    "    # print report\n",
    "    print(\n",
    "        f'Frequency quantile {q_:.2f} = {df_rfm[\"frequency\"].quantile(q=q_, interpolation=\"linear\"):.0f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a37b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency -> there are many quantiles have same frequency values\n",
    "# it is a problem if we try to split into this quantiles.\n",
    "# As frequency distribution is skew -> split using business understanding, not quantiles/percentiles\n",
    "\n",
    "# mapping frequency values into F customer 'labels'\n",
    "# F = 1 if: frequency = 1\n",
    "# F = 2 if:  1 < frequency <= 2\n",
    "# F = 3 if:  2 < frequency <= 4\n",
    "# F = 4 if:  4 < frequency <= 7 \n",
    "# F = 5 if:  frequency > 7\n",
    "# create bins of the mapping\n",
    "m_bins = [ 0, 1, 2, 4, 7, 100 ]\n",
    "# create labels of the mapping\n",
    "m_labels = (0, 1, 2, 3, 4)\n",
    "# calculate values to cut F into bins into discrete intervals.\n",
    "frequency_cut = pd.cut( \n",
    "    x=df_rfm['frequency'], \n",
    "    bins=m_bins, labels=m_labels,\n",
    "    right=True, retbins=True, include_lowest=False, ordered=True\n",
    "    )\n",
    "# map frequency feature into F feature\n",
    "df_rfm['F'] = frequency_cut[0].tolist()\n",
    "# save bins limits\n",
    "rfm_transform['F'] = {\"right\": True, \"include_lowest\": False, \"bins\": frequency_cut[1]}\n",
    "\n",
    "# sanity check\n",
    "assert len(df_rfm[\"F\"].unique()) == 5, \"Missing F label on preprocessing\"\n",
    "\n",
    "# inspect results\n",
    "display(\n",
    "    rfm_transform, \n",
    "    df_rfm.sample(20, random_state=7)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e328e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1 to R, M and F -> easier to interpret\n",
    "df_rfm[[\"R\", \"M\", \"F\"]] = df_rfm[[\"R\", \"M\", \"F\"]] + 1\n",
    "\n",
    "# calculate mean RFM for each customer\n",
    "df_rfm['avg_RFM'] = df_rfm[['R', 'F', 'M']].mean(axis = 1)\n",
    "\n",
    "# inspect results\n",
    "check_dataframe(df_rfm, summary_stats=True, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22943d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some noise to RFM so as to better visualize it\n",
    "for col in [\"R\", \"M\", \"F\"]:\n",
    "    # create a new column with a normal noise\n",
    "    df_rfm[f\"{col}_noisy\"] = df_rfm[col] + np.random.normal(loc=0, scale=0.3, size=len(df_rfm))\n",
    "\n",
    "# inspect results\n",
    "df_rfm.sample(10, random_state=7)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc7b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualy inspect clusters\n",
    "sns.scatterplot(data=df_rfm.sample(10_000), \n",
    "                x=\"R_noisy\", y=\"F_noisy\", \n",
    "                hue=\"M_noisy\", hue_order=[0, 1, 2, 3, 4],\n",
    "                alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4841e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataframe\n",
    "check_dataframe(df_rfm, summary_stats=True, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb55168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bins of the mapping\n",
    "m_bins = [ 1, 2, 3, 4, 5 ]\n",
    "# create labels of the mapping\n",
    "m_labels = (1, 2, 3, 4)\n",
    "\n",
    "# calculate values to cut RFM into bins into discrete intervals.\n",
    "rfm_cut = pd.cut( \n",
    "    x=df_rfm['avg_RFM'], \n",
    "    bins=m_bins, labels=m_labels,\n",
    "    right=True, retbins=True, include_lowest=True, ordered=True\n",
    "    )\n",
    "# map frequency feature into F feature\n",
    "df_rfm['cluster'] = rfm_cut[0].tolist()\n",
    "# save bins limits\n",
    "rfm_transform['RFM'] = {\"right\": True, \"include_lowest\": False, \"bins\": rfm_cut[1]}\n",
    "\n",
    "# inspect dataframe\n",
    "df_rfm.sample(20, random_state=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b37913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect dataframe\n",
    "check_dataframe(df_rfm, summary_stats=True, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b24f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cluster description\n",
    "df_cluster = df_rfm.groupby(by=\"cluster\", as_index=False).agg(\n",
    "    cluster_avg_recency = (\"recency\", \"mean\"),\n",
    "    cluster_avg_frequency = (\"frequency\", \"mean\"),\n",
    "    cluster_avg_monetary = (\"monetary\", \"mean\"),\n",
    "    cluster_size = (\"buyer_id\", \"nunique\")\n",
    ")\n",
    "\n",
    "# get RFM for clusters mean\n",
    "df_cluster[\"clusters_r\"] = np.digitize(x=df_cluster[\"cluster_avg_recency\"], bins=rfm_transform[\"R\"][\"bins\"], right=True)\n",
    "df_cluster[\"clusters_f\"] = np.digitize(x=df_cluster[\"cluster_avg_frequency\"], bins=rfm_transform[\"F\"][\"bins\"], right=True)\n",
    "df_cluster[\"clusters_m\"] = np.digitize(x=df_cluster[\"cluster_avg_monetary\"], bins=rfm_transform[\"M\"][\"bins\"], right=True)\n",
    "\n",
    "# inspect results\n",
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average RFM for each cluster\n",
    "sns.scatterplot(data = df_cluster, x = 'cluster_avg_frequency', y = 'cluster_avg_recency', \n",
    "                size = 'cluster_size', hue = 'cluster_avg_monetary',\n",
    "                palette=\"tab10\"\n",
    "                )\n",
    "# plot details\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.ylim(0, 150)\n",
    "plt.xlim(0.5, 6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652802eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info from cluster percent size\n",
    "df_cluster['cluster_size_percent'] = ( df_cluster['cluster_size'] / df_cluster['cluster_size'].sum() ) * 100\n",
    "\n",
    "# give friendly name to clusters\n",
    "df_cluster['cluster'] = df_cluster['cluster'].map({\n",
    "    1: 'Winter', \n",
    "    2: 'Autumn', \n",
    "    3: 'Spring', \n",
    "    4: 'Summer'\n",
    "} )\n",
    "\n",
    "# inspect results\n",
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae489545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "df_cluster.to_csv(\n",
    "    os.path.join(PROJECT_ROOT_PATH, \"artifacts\", \"rfm\", \"df_cluster.csv\"),\n",
    "    header=True, index=False, mode=\"w\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over clusters\n",
    "for cluster in [\"Summer\", \"Spring\", \"Autumn\", \"Winter\"]:\n",
    "    # get info for report\n",
    "    num_customers = df_cluster.loc[df_cluster[\"cluster\"]==cluster, \"cluster_size\"].tolist()[0]\n",
    "    num_customers_percent = df_cluster.loc[df_cluster[\"cluster\"]==cluster, \"cluster_size_percent\"].tolist()[0]\n",
    "    avg_recency = df_cluster.loc[df_cluster[\"cluster\"]==cluster, \"cluster_avg_recency\"].tolist()[0]\n",
    "    avg_frequency = df_cluster.loc[df_cluster[\"cluster\"]==cluster, \"cluster_avg_frequency\"].tolist()[0]\n",
    "    avg_monetary = df_cluster.loc[df_cluster[\"cluster\"]==cluster, \"cluster_avg_monetary\"].tolist()[0]\n",
    "\n",
    "    # print report\n",
    "    # print final report\n",
    "    print(\n",
    "        f\"{cluster.upper()} cluster:\\n\"\n",
    "        f\"\\tnumber of customers: {num_customers} ({num_customers_percent:.2f}% of customers)\\n\"\n",
    "        f\"\\taverage recency:     {int(avg_recency)} days (days since last purchase)\\n\"\n",
    "        f\"\\taverage frequency:   {avg_frequency:.1f} purchases (in average)\\n\"\n",
    "        f\"\\taverage money spent: {avg_monetary:.1f} (average spent per customer in Z-score)\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rfm mapping\n",
    "rfm_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79123c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bins mapping\n",
    "with open(os.path.join(PROJECT_ROOT_PATH, \"artifacts\", \"rfm\", \"rfm_map.pickle\"), \"wb\") as f:\n",
    "    # save mapping as pickle\n",
    "    pickle.dump(\n",
    "        obj=rfm_transform,\n",
    "        file=f,\n",
    "        protocol= pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "\n",
    "# save bins mapping\n",
    "with open(os.path.join(PROJECT_ROOT_PATH, \"artifacts\", \"rfm\", \"rfm_map.pickle\"), \"rb\") as f:\n",
    "    # save mapping as pickle\n",
    "    rfm_transform = pickle.load(\n",
    "        file=f,\n",
    "    )\n",
    "\n",
    "# check mapping bins\n",
    "rfm_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064f530",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "####### check streamlit interface ######\n",
    "########################################\n",
    "\n",
    "r = 100\n",
    "f = 1.5\n",
    "m = -0.2\n",
    "\n",
    "customer_r = np.digitize(x=r, bins=rfm_transform[\"R\"][\"bins\"], right=True)\n",
    "customer_f = np.digitize(x=f, bins=rfm_transform[\"F\"][\"bins\"], right=True)\n",
    "customer_m = np.digitize(x=m, bins=rfm_transform[\"M\"][\"bins\"], right=True)\n",
    "\n",
    "display(\"rfm map:\", rfm_transform)\n",
    "\n",
    "print(\n",
    "    f\"Customer R: {customer_r}\\n\"\n",
    "    f\"Customer F: {customer_f}\\n\"\n",
    "    f\"Customer M: {customer_m}\\n\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3cf4e43",
   "metadata": {},
   "source": [
    "**Streamlit app to validate use of RFM segmentation**:\n",
    "\n",
    "- https://ds-gustavo-cunha-hotmart-case-streamlit-app-rddkkh.streamlit.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous section dataframe to save memory\n",
    "lst = [df_cluster, df_rfm, recency_qcut]\n",
    "del lst\n",
    "del df_cluster\n",
    "del df_rfm\n",
    "del recency_qcut"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1671d285",
   "metadata": {},
   "source": [
    "### **H4.** It is possible to **estimate** how much **revenue** Hotmart will generate in the **next three months from the last month** shown in the dataset?\n",
    "        - Check the revenue time-series to understand how to extrapolate it to the future\n",
    "            - Visual inspection\n",
    "            - Check for trend and seasonality and noise\n",
    "            - Define baseline (dummy = last available date)\n",
    "                - Initially, ARIMA model\n",
    "                - If possible, machine learning models\n",
    "                - Check model error and extrapolate to business impact\n",
    "                  - knowing revenue forecast we can predcit scaling investments and even prepone investments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f0afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect datetime columns\n",
    "datetime_summary(df_eda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6c12484",
   "metadata": {},
   "source": [
    "*There are only 6 months of purchase history so it's quite complex to predict future purchases for the next months with such few amount of data (how to extract trend, seasonality and so on?).*\n",
    "\n",
    "**We will try to use a daily purchase frequency instead to workaround the lack of data in monthly frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4612241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby date and get meaningful agg for each feature\n",
    "df_daily_purchases_agg = df_eda.groupby(\n",
    "    by=[\"purchase_date_daily\"], as_index=False).agg(\n",
    "    sum_purchase_value =(\"purchase_value\", \"sum\"),\n",
    "    mean_product_age_when_purchased = (\"product_age_when_purchased\", \"mean\"),\n",
    "    mean_affiliate_commission_percentual = (\"affiliate_commission_percentual\", \"mean\"),\n",
    "    nunique_purchase_id = (\"purchase_id\", \"nunique\"),\n",
    "    nunique_product_id = (\"product_id\", \"nunique\"),\n",
    "    nunique_affiliate_id = (\"affiliate_id\", \"nunique\"),\n",
    "    nunique_buyer_id = (\"buyer_id\", \"nunique\"),\n",
    "    nunique_product_category = (\"product_category\", \"nunique\"),\n",
    "    nunique_product_niche = (\"product_niche\", \"nunique\"),\n",
    "    nunique_purchase_device = (\"purchase_device\", \"nunique\"),\n",
    "    nunique_is_origin_page_social_network = (\"is_origin_page_social_network\", \"nunique\")\n",
    ")\n",
    "\n",
    "# define purchase_date_daily as index and set its frequency to daily\n",
    "df_daily_purchases_agg = df_daily_purchases_agg.set_index(\"purchase_date_daily\").asfreq(\"D\")\n",
    "\n",
    "# interpolate any missing value on the time series\n",
    "df_daily_purchases_agg = df_daily_purchases_agg.interpolate(method='linear', axis=0)\n",
    "\n",
    "# inspect preprocessing\n",
    "df_daily_purchases_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8080bdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over columns\n",
    "for col in df_daily_purchases_agg.columns.tolist():\n",
    "    # print report\n",
    "    print(\n",
    "        f\"{'*'*49}\\n\"\n",
    "        f\"\\t{col.upper()}\\n\"\n",
    "        f\"{'*'*49}\\n\"\n",
    "    )\n",
    "    # plot seasonal decompose\n",
    "    seasonal_decompose(df_daily_purchases_agg[col], model='additive').plot()\n",
    "    # plot details\n",
    "    g = plt.gcf()\n",
    "    g.set_size_inches(12, 7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect again target variable\n",
    "sd = seasonal_decompose(df_daily_purchases_agg[\"sum_purchase_value\"], model='additive')\n",
    "sd.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a81290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually inspect residuals\n",
    "plt.plot(sd.resid)\n",
    "plt.title(\"Additive seasonal decompose residuals\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e40459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform Augmented Dickey Fuller - ADF to check for stationarity\n",
    "# H0: The series is not-stationary\n",
    "adfuller(df_daily_purchases_agg[\"sum_purchase_value\"])[1]  # p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define figure layout\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2)\n",
    "# plot acf and padf\n",
    "plot_acf(df_daily_purchases_agg[\"sum_purchase_value\"], c=\"r\", zero=True, ax=axs[0])\n",
    "plot_pacf(df_daily_purchases_agg[\"sum_purchase_value\"], c=\"y\", zero=True, ax=axs[1])\n",
    "# plot details\n",
    "axs[0].set_ylim(-0.5, 1.05)\n",
    "axs[1].set_ylim(-0.5, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1868d606",
   "metadata": {},
   "source": [
    "#### *ARIMA*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5a8a494",
   "metadata": {},
   "source": [
    "In summary, initial guess is:\n",
    "\n",
    "- **p = 1, q = 2, i = 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40340d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model dataset\n",
    "df_modelling = df_daily_purchases_agg[\"sum_purchase_value\"]\n",
    "\n",
    "# define training ratio and split index\n",
    "# tr = 0.75\n",
    "# split_index = int(len(df_modelling)*0.7)\n",
    "split_index = -30\n",
    "\n",
    "# split dataset\n",
    "train = df_modelling.iloc[:split_index]\n",
    "test = df_modelling.iloc[split_index:]\n",
    "\n",
    "# inspect results\n",
    "print(\n",
    "    f\"Train:\\n\\tmin={train.index.min()}; \\n\\tmax={train.index.max()}\\n\\n\"\n",
    "    f\"Test: \\n\\tmin={test.index.min()}; \\n\\tmax={test.index.max()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ARIMA model\n",
    "arima = ARIMA(\n",
    "    endog=train, \n",
    "    exog=None, \n",
    "    order=(1, 0, 2), \n",
    "    seasonal_order=(0, 0, 0, 0), \n",
    "    trend=None, \n",
    "    enforce_stationarity=True, \n",
    ")    \n",
    "\n",
    "# fit model to data\n",
    "arima = arima.fit()\n",
    "# check results\n",
    "arima.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b695b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plot layout\n",
    "fig, axs = plt.subplots(1, 1, figsize=(12, 5))\n",
    "# plot real values\n",
    "axs.plot(df_daily_purchases_agg[\"sum_purchase_value\"], label='observed')\n",
    "# plot predicted values\n",
    "plot_predict(arima, start=1, end=180, ax=axs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b8700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform error analysis\n",
    "arima.plot_diagnostics(figsize=(12,10))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3ffa6a3",
   "metadata": {},
   "source": [
    "#### *Auto-ARIMA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abf2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Auto-ARIMA to automatically discover the optimal order for an ARIMA model.\n",
    "auto_model = pm.auto_arima(train,\n",
    "                    start_p=0, max_p=2,\n",
    "                    d=0, max_d=0, \n",
    "                    start_q=0, max_q=2,\n",
    "                    start_P=0, max_P=1, \n",
    "                    D=1, max_D=1,\n",
    "                    start_Q=0, max_Q=2,\n",
    "                    max_order=5, \n",
    "                    seasonal=True, m=7, # weekly seasonality\n",
    "                    trend=None, with_intercept=\"auto\",\n",
    "                    #out_of_sample_size=7, \n",
    "                    scoring=\"mse\",\n",
    "                    trace=True, error_action=\"ignore\", suppress_warnings=True, \n",
    "                    n_jobs=-1\n",
    "                    )\n",
    "\n",
    "# try to train model\n",
    "try:\n",
    "    # fit mode to data\n",
    "    auto_model.fit()\n",
    "\n",
    "# in case of type error    \n",
    "except TypeError:\n",
    "    pass\n",
    "\n",
    "# regardless of errors\n",
    "finally:\n",
    "   # inspect results\n",
    "    display(auto_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94447888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plot layout\n",
    "fig, axs = plt.subplots(1, 1, figsize=(12, 5))\n",
    "# plot real values\n",
    "axs.plot(df_daily_purchases_agg[\"sum_purchase_value\"], label='Real')\n",
    "# plot ARIMA predicted values\n",
    "plot_predict(arima, start=1, end=180, ax=axs, label=\"ARIMA\");\n",
    "# plot SARIMA\n",
    "plt.plot(auto_model.predict(len(test)), label=\"SARIMA\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c72df8b5",
   "metadata": {},
   "source": [
    "#### *Profet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model dataset\n",
    "df_modelling = df_daily_purchases_agg[\"sum_purchase_value\"]\n",
    "# set index as ds column\n",
    "df_modelling = df_modelling.reset_index().rename(columns={\"purchase_date_daily\": \"ds\", \"sum_purchase_value\": \"y\"})\n",
    "\n",
    "# define training ratio and split index\n",
    "# tr = 0.75\n",
    "# split_index = int(len(df_modelling)*0.7)\n",
    "split_index = -30\n",
    "\n",
    "# split dataset\n",
    "train = df_modelling.iloc[:split_index]\n",
    "test = df_modelling.iloc[split_index:]\n",
    "\n",
    "# instanciate model\n",
    "model = Prophet(seasonality_mode='additive')\n",
    "\n",
    "# fit model to data\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ac35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make forecasts for testing data\n",
    "test_forecast = model.predict(test)\n",
    "test_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74095c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot forecast\n",
    "model.plot(test_forecast);\n",
    "plt.plot(train[\"ds\"], train[\"y\"], label=\"train\", color=\"r\", alpha=0.5)\n",
    "plt.plot(test[\"ds\"], test[\"y\"], label=\"test\", color=\"black\", linestyle=\"dotted\")\n",
    "plt.plot(auto_model.predict(len(test)), color=\"y\", label=\"SARIMA\")\n",
    "plt.legend()\n",
    "#plt.xlim(train.index.min()a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e7fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction components\n",
    "model.plot_components(test_forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b60875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validate model\n",
    "df_cv = cross_validation(\n",
    "    model=model, \n",
    "    initial = \"30 days\", # size of the initial training period\n",
    "    period = \"15 days\", # spacing between cutoff dates\n",
    "    horizon = \"30 days\", # forecast horizon  \n",
    "    )\n",
    "\n",
    "# inspect results\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get performance metrics\n",
    "df_performance = performance_metrics(df_cv)\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mape performance\n",
    "plot_cross_validation_metric(df_cv, metric='mape')\n",
    "plt.ylim(0, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f71be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete previous section dataframe to save memory\n",
    "lst = [df_daily_purchases_agg]\n",
    "del lst\n",
    "del df_daily_purchases_agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "681c01c5",
   "metadata": {},
   "source": [
    "### **H5.** What **features most impact** the success of a **product**? that is, what makes a **product sell more**?\n",
    "        - Success of a product = number of products sold\n",
    "            - Inspect features with high correlation to the number of product sold\n",
    "            - Inspect feature with high correlation with an increasing trend of products sold\n",
    "            - Check for simple causal inference techniques\n",
    "              - knowing features that best impact the product success, we can use this feature for marketing purpose (scalling effort) and, perhaps, get a better overview about what leads to focus on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c72585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect source dataframe\n",
    "check_dataframe(df_eda, summary_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect unique purchase devices\n",
    "display(df_eda[\"purchase_device\"].unique())\n",
    "\n",
    "# convert purchase_device to dummies\n",
    "df_purchase_device_dummies = pd.get_dummies(data=df_eda[\"purchase_device\"], \n",
    "                                            prefix=\"purchase_device\", \n",
    "                                            prefix_sep='_', \n",
    "                                            dummy_na=False, \n",
    "                                            columns=None, \n",
    "                                            sparse=False, \n",
    "                                            drop_first=False, \n",
    "                                            dtype=bool               \n",
    "                                            )\n",
    "\n",
    "# inspect results\n",
    "df_purchase_device_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c965d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of eda dataframe\n",
    "df_product = df_eda.copy()\n",
    "\n",
    "# sanity check\n",
    "assert len(df_product) == len(df_purchase_device_dummies), \"Purchase dummies dataframe miss data!\"\n",
    "\n",
    "# concatenate purchase_device dummies\n",
    "df_product[df_purchase_device_dummies.columns.str.lower()] = df_purchase_device_dummies\n",
    "df_product = df_product.rename(columns={\"purchase_device_smart tv\": \"purchase_device_smart_tv\"})\n",
    "\n",
    "# inspect results\n",
    "df_product.sample(5, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112978b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate meaning information per product\n",
    "df_product_agg = df_product.sort_values(\n",
    "        by=[\"purchase_date\"], ignore_index=True # order dataframe by purchase data before aggregating\n",
    "    ).groupby(\n",
    "        by=\"product_id\", as_index=False\n",
    "    ).agg(\n",
    "    nunique_buyer_id = (\"buyer_id\", \"nunique\"),\n",
    "    nunique_purchase_id = (\"purchase_id\", \"nunique\"), \n",
    "    sum_purchase_value = (\"purchase_value\", \"sum\"),\n",
    "    nunique_affiliate_id = (\"affiliate_id\", \"nunique\"),\n",
    "    nunique_producer_id = (\"producer_id\", \"nunique\"),\n",
    "    last_purchase_date = (\"purchase_date\", \"max\"),\n",
    "    min_product_creation_date = (\"product_creation_date\", \"min\"), \n",
    "    last_product_category = (\"product_category\", \"last\"),\n",
    "    last_product_niche = (\"product_niche\", \"last\"),\n",
    "    mean_affiliate_commission_percentual = (\"affiliate_commission_percentual\", \"mean\"),\n",
    "    mean_purchase_device_cellphone = (\"purchase_device_cellphone\", \"mean\"),\n",
    "    mean_purchase_device_desktop = (\"purchase_device_desktop\", \"mean\"),\n",
    "    mean_purchase_device_smart_tv = (\"purchase_device_smart_tv\", \"mean\"),\n",
    "    mean_purchase_device_tablet = (\"purchase_device_tablet\", \"mean\"),\n",
    "    mean_purchase_device_ereaders = (\"purchase_device_ereaders\", \"mean\"),\n",
    "    mean_is_origin_page_social_network = (\"is_origin_page_social_network\", \"mean\"),\n",
    "    mean_product_age_when_purchased = (\"product_age_when_purchased\", \"mean\"),\n",
    ")\n",
    "\n",
    "# inspect results\n",
    "df_product_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4176e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate summary stats\n",
    "check_dataframe(df_product_agg, summary_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c985fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature to use for clustering\n",
    "cluster_cols = ['nunique_buyer_id', 'nunique_purchase_id',\n",
    "       'sum_purchase_value', 'nunique_affiliate_id', 'nunique_producer_id',\n",
    "       #'last_purchase_date', 'min_product_creation_date', # datetime\n",
    "       # 'last_product_category', 'last_product_niche', # categorical\n",
    "       'mean_affiliate_commission_percentual', # remember -1 flag\n",
    "       'mean_purchase_device_cellphone', 'mean_purchase_device_desktop',\n",
    "       'mean_purchase_device_smart_tv', 'mean_purchase_device_tablet',\n",
    "       'mean_purchase_device_ereaders', 'mean_is_origin_page_social_network',\n",
    "       'mean_product_age_when_purchased']\n",
    "\n",
    "# define modelling dataframe\n",
    "df_modelling = df_product_agg[cluster_cols]#.sample(100_000, random_state=7)       \n",
    "# inspect \n",
    "df_modelling.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define default params for model instanciation\n",
    "# models_to_try = {\n",
    "#     \"kmeans\": (\n",
    "#         KMeans, \n",
    "#         dict(\n",
    "#             init='k-means++', \n",
    "#             n_init=100, \n",
    "#             max_iter=300, \n",
    "#             verbose=0, \n",
    "#             random_state=None, \n",
    "#             copy_x=True\n",
    "#             )\n",
    "#         ), \n",
    "#     \"gmixture\": (\n",
    "#         GaussianMixture,\n",
    "#         dict(\n",
    "#             covariance_type='full', \n",
    "#             max_iter=300, \n",
    "#             n_init=100, \n",
    "#             init_params='k-means++', \n",
    "#             random_state=None, \n",
    "#             warm_start=False, \n",
    "#             verbose=0\n",
    "#             )\n",
    "#         ),\n",
    "#     \"dbscan\": (\n",
    "#         DBSCAN,\n",
    "#         dict(\n",
    "#             min_samples=5, \n",
    "#             metric='euclidean', \n",
    "#             algorithm='auto', \n",
    "#             p=None, \n",
    "#             n_jobs=-1\n",
    "#             )\n",
    "#     )\n",
    "#     }\n",
    "\n",
    "# # define params to try\n",
    "# params_to_try = {\n",
    "#     \"kmeans\": dict(n_clusters=[*np.arange( 2, 11)]),\n",
    "#     \"gmixture\": dict(n_components=[*np.arange(2, 11)]),\n",
    "#     \"dbscan\": dict(eps=[*np.arange(0.1, 1.0, 0.1)])\n",
    "# }\n",
    "\n",
    "# # try to create experiment\n",
    "# try:\n",
    "#     # create MLFlow experiment\n",
    "#     experiment_id = mlflow.create_experiment(\n",
    "#         name=\"initial_clustering_inspection\"\n",
    "#         )\n",
    "# # experiment already exist\n",
    "# except mlflow.exceptions.MlflowException:\n",
    "#     # create MLFlow experiment\n",
    "#     experiment_id = mlflow.get_experiment_by_name(name=\"initial_clustering_inspection\").experiment_id\n",
    "\n",
    "\n",
    "# # iterate over models to test\n",
    "# for model_tag in models_to_try.keys():\n",
    "#     # define params to inspect for the given model\n",
    "#     params_to_inspect = params_to_try[model_tag]\n",
    "#     # get param name and param values\n",
    "#     param_name = list(params_to_inspect.keys())[0]\n",
    "#     param_values = list(params_to_inspect.values())[0]\n",
    "\n",
    "#     # iterate over params to test\n",
    "#     for run_param in param_values:\n",
    "\n",
    "#         # print report\n",
    "#         print(f\"On model {model_tag.upper()} with {param_name} = {run_param} 🚀\")\n",
    "\n",
    "#         # define scaler for numeric features\n",
    "#         rs = RobustScaler(\n",
    "#             with_centering=True,\n",
    "#             with_scaling=True,\n",
    "#             quantile_range=(25.0, 75.0)\n",
    "#             )\n",
    "\n",
    "#         # define sklearn pipeline\n",
    "#         preprocessing_pipe = make_pipeline(rs)\n",
    "\n",
    "#         # define model variable and its params\n",
    "#         model = models_to_try[model_tag][0]\n",
    "#         iteration_params = models_to_try[model_tag][1].copy()\n",
    "#         iteration_params[ param_name ] = run_param\n",
    "\n",
    "#         # print model params\n",
    "#         print(f\"\\t{iteration_params}\")\n",
    "\n",
    "#         # define ML model\n",
    "#         ml_model = model(**iteration_params)\n",
    "\n",
    "#         # add ML model to pipelin\n",
    "#         full_pipe = Pipeline([\n",
    "#             (\"preprocess\", preprocessing_pipe),\n",
    "#             (\"skl_model\", ml_model)\n",
    "#         ])\n",
    "\n",
    "#         # fit model to data and get labels for each point in data space\n",
    "#         labels = full_pipe.fit_predict(df_modelling)\n",
    "\n",
    "#         # get average silhouette score the the given number of clusters\n",
    "#         s_score = silhouette_score( \n",
    "#             X=df_modelling, \n",
    "#             labels=labels, \n",
    "#             metric='euclidean', \n",
    "#             sample_size=None # use all datapoints\n",
    "#         )\n",
    "\n",
    "#         # define MLFlow model tag\n",
    "#         mlflow_model_tag = f\"{model_tag.upper()}({param_name}={run_param})\"\n",
    "\n",
    "#         # make a run on MLFlow with context manager\n",
    "#         with mlflow.start_run(experiment_id=experiment_id, \n",
    "#                          run_name=mlflow_model_tag, \n",
    "#                          tags={\n",
    "#                              \"experiment_type\": \"silhouette_inspection\",\n",
    "#                              \"model\": model_tag\n",
    "#                              }):\n",
    "#             # select active run\n",
    "#             run = mlflow.active_run()\n",
    "#             # report run status\n",
    "#             print(f\"\\texperiment_id: {experiment_id}; run_id: {run.info.run_id}; status: {run.info.status}\")\n",
    "\n",
    "#             # log model name as param\n",
    "#             mlflow.log_param(\"model\", mlflow_model_tag)\n",
    "\n",
    "#             # log metrics on MLFlow\n",
    "#             mlflow.log_metric(\"silhouette_score\", s_score)\n",
    "#             # log chosen model params\n",
    "#             mlflow.log_params(iteration_params)\n",
    "\n",
    "#             # log model\n",
    "#             mlflow.sklearn.log_model(\n",
    "#                 sk_model=full_pipe, \n",
    "#                 artifact_path=\"sklearn_clustering\"\n",
    "#                 )\n",
    "\n",
    "#             # end run and get status\n",
    "#             mlflow.end_run()\n",
    "#             run = mlflow.get_run(run.info.run_id)\n",
    "#             print(f\"\\texperiment_id: {experiment_id}; run_id: {run.info.run_id}; status: {run.info.status}\")\n",
    "\n",
    "#             # print report\n",
    "#             print(f\"Successfully trained and logged {model_tag.upper()} with {param_name} = {run_param} ✅\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0efb0c40",
   "metadata": {},
   "source": [
    "**REVISE**\n",
    "\n",
    "num_compras\n",
    "valor_compra\n",
    "\n",
    "valor / num -> alto -> compras de alto valor\n",
    "\n",
    "(np.log1p(df_product_agg[\"nunique_buyer_id\"])*np.log1p(df_product_agg[\"nunique_purchase_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# based on MLFlow analysis\n",
    "# initial model = KMeans with 7 clusters\n",
    "########################################\n",
    "\n",
    "\n",
    "# define scaler for numeric features\n",
    "rs = RobustScaler(\n",
    "    with_centering=True,\n",
    "    with_scaling=True,\n",
    "    quantile_range=(25.0, 75.0)\n",
    "    )\n",
    "\n",
    "# define sklearn pipeline\n",
    "preprocessing_pipe = make_pipeline(rs)\n",
    "\n",
    "# define model variable and its params\n",
    "model_final_params=dict(\n",
    "    n_clusters=7,\n",
    "    init='k-means++', \n",
    "    n_init=100, \n",
    "    max_iter=300, \n",
    "    verbose=0, \n",
    "    random_state=None, \n",
    "    copy_x=True\n",
    "            )\n",
    "\n",
    "# add ML model to pipelin\n",
    "full_pipe = Pipeline([\n",
    "    (\"preprocess\", preprocessing_pipe),\n",
    "    (\"skl_model\", KMeans(**model_final_params))\n",
    "])\n",
    "\n",
    "# fit model to data and get labels for each point in data space\n",
    "full_pipe.fit(df_modelling)\n",
    "\n",
    "# get preprocessed modeling data\n",
    "df_modelling_preprocessed = pd.DataFrame(\n",
    "    data=full_pipe.get_params()[\"steps\"][0][1].transform(df_modelling),\n",
    "    columns=full_pipe.get_params()[\"steps\"][0][1].get_feature_names_out()\n",
    "    )\n",
    "\n",
    "# sanity check\n",
    "assert df_modelling_preprocessed.shape == df_modelling.shape, \"Using pipelines incorrectly!\"\n",
    "\n",
    "# calculate silhouette score for individual datapoints\n",
    "samples_silhouette_values = silhouette_samples( \n",
    "    X=df_modelling_preprocessed, \n",
    "    labels=full_pipe.predict(df_modelling), \n",
    "    metric='euclidean'\n",
    "    )\n",
    "\n",
    "# sanity check\n",
    "assert len(samples_silhouette_values) == len(df_modelling), \"Missing data on silhouette calculation!\"\n",
    "\n",
    "# inspect results\n",
    "samples_silhouette_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c2bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect silhouette\n",
    "silhouette_inspection_pipelined(\n",
    "    dataframe=df_modelling_preprocessed,\n",
    "    labels=full_pipe.predict(df_modelling)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a83c4df3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 5.5 - Data Space Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "035779a8",
   "metadata": {},
   "source": [
    "**Initial inspection on dimensionality reduction potential**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58d17b6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T14:52:05.470744Z",
     "start_time": "2021-11-12T14:52:05.466937Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918b2c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:35:39.981919Z",
     "start_time": "2021-11-22T11:35:39.955021Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No need so far"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cfeafc6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1451b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:36:07.808671Z",
     "start_time": "2021-11-22T11:35:40.592638Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No need so far"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "158cdc9e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4ccb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:36:45.325959Z",
     "start_time": "2021-11-22T11:36:08.051724Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# No need so far"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "facedc5f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### PHATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2ab66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:37:16.617873Z",
     "start_time": "2021-11-22T11:36:45.530058Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No need so far"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dad2f694",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### KMeans Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156840e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:37:40.479163Z",
     "start_time": "2021-11-22T11:37:29.524697Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# No need so far"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f1f4718",
   "metadata": {},
   "source": [
    "### Tree-Base Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bdd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need so far"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c42ecba",
   "metadata": {},
   "source": [
    "# **6 - DATA PREPARATION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec7eabd2",
   "metadata": {},
   "source": [
    "## 6.1 - Restore Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5eabd39",
   "metadata": {},
   "source": [
    "*Create a checkpoint of the last dataframe from previous section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a restore point for the previous section dataframe\n",
    "df_prep = df_eda.copy()\n",
    "\n",
    "# check dataframe\n",
    "check_dataframe( df_prep )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44c0d35a",
   "metadata": {},
   "source": [
    "## 6.2 - Remove variables that won't be available in the production environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "767334b4",
   "metadata": {},
   "source": [
    "*Remove variables that model can use on production to make predictions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cfe39a4",
   "metadata": {},
   "source": [
    "## 6.3 - Train-Validation-Test split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a14a8105",
   "metadata": {},
   "source": [
    "*Split dataframe into training, validation and test dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac0f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2f48e31",
   "metadata": {},
   "source": [
    "## 6.4 - Scale numeric features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd93f108",
   "metadata": {},
   "source": [
    "*Scale numeric feature to make modelling \"easier\" for ML models*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c58a8f54",
   "metadata": {},
   "source": [
    "### 6.4.1 - Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfe8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f40f60b7",
   "metadata": {},
   "source": [
    "### 6.4.2 - Min-Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d1dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "880503b8",
   "metadata": {},
   "source": [
    "### 6.4.3 - Robust Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a24aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22c6163d",
   "metadata": {},
   "source": [
    "### 6.4.4 - Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f78a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bed93774",
   "metadata": {},
   "source": [
    "## 6.5 - Encode categorical features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55ac6f45",
   "metadata": {},
   "source": [
    "*Encode categorical feature to make modelling possible for ML models*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0814e8c",
   "metadata": {},
   "source": [
    "### 6.5.1 - One-Hot Encodingm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2486ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e50dacee",
   "metadata": {},
   "source": [
    "### 6.5.2 - Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5848643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d8e7c71",
   "metadata": {},
   "source": [
    "### 6.5.3 - Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d40c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99e12393",
   "metadata": {},
   "source": [
    "## 6.6 - Response variable transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f07db9d",
   "metadata": {},
   "source": [
    "*Transform target variable (e.g. log, sqrt, etc) to make modelling \"easier\" for ML models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20a1fee9",
   "metadata": {},
   "source": [
    "## 6.7 - Cyclic variables transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88bdcde",
   "metadata": {},
   "source": [
    "*Transform cyclic variables (e.g. days of week, months in year, etc) with a sin and cos functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa224055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35474f40",
   "metadata": {},
   "source": [
    "## 6.8 - Double-check preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec4faaa7",
   "metadata": {},
   "source": [
    "*Double-check the prepared dataset to make sure it is as expected*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f62edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee9d757b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T11:28:02.993277Z",
     "start_time": "2021-11-09T11:28:02.988140Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# **7 - FEATURE SELECTION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "989cfb33",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 7.1 - Restore Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dab40a87",
   "metadata": {},
   "source": [
    "*Create a checkpoint of the last dataframe from previous section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e572e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a restore point for the previous section dataframe\n",
    "df_f_selection = df_prep.copy()\n",
    "\n",
    "# check dataframe\n",
    "check_dataframe( df_f_selection )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ed869c5",
   "metadata": {},
   "source": [
    "## 7.2 - Logist regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18ded795",
   "metadata": {},
   "source": [
    "## 7.3 - Random forest feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed578042",
   "metadata": {},
   "source": [
    "## 7.4 - Boruta algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9be4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "593aa50b",
   "metadata": {},
   "source": [
    "## 7.5 - Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42990ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cd4a7d0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# **8 - ML MODEL TRAINING**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b3baa9b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 8.1 - Restore Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78eb6db9",
   "metadata": {},
   "source": [
    "*Create a checkpoint of the last dataframe from previous section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711c6a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T11:44:02.805951Z",
     "start_time": "2021-11-22T11:44:02.715657Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a restore point for the previous section dataframe\n",
    "df_train = df_f_selection.copy()\n",
    "\n",
    "# check dataframe\n",
    "check_dataframe( df_train )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dab90bd1",
   "metadata": {},
   "source": [
    "## 8.2 - Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3265dbc",
   "metadata": {},
   "source": [
    "*Define the metric of success and the health metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "611df983",
   "metadata": {},
   "source": [
    "## 8.3 - Baseline model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1cba9e3",
   "metadata": {},
   "source": [
    "*Check the performance metrics with a dummy model to get the baseline metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74687ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca3fc080",
   "metadata": {},
   "source": [
    "## 8.4 - ML models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d1afd08",
   "metadata": {},
   "source": [
    "*Get performance metrics of ML model with cross-validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d3c3c07",
   "metadata": {},
   "source": [
    "## 8.5 - Final modelling comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd70b6db",
   "metadata": {},
   "source": [
    "*Compare all models and decide what one is the best (and will be fine-tuned)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4712c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "575611fe",
   "metadata": {},
   "source": [
    "# **9 - HYPERPARAMETER TUNNING**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2edc6bc",
   "metadata": {},
   "source": [
    "## 9.1 - Restore Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc71366d",
   "metadata": {},
   "source": [
    "*Create a checkpoint of the last dataframe from previous section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a restore point for the previous section dataframe\n",
    "df_tune = df_train.copy()\n",
    "\n",
    "# check dataframe\n",
    "check_dataframe( df_tune )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de10e50a",
   "metadata": {},
   "source": [
    "## 9.2 - Hypertune the best ML model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e6ac096",
   "metadata": {},
   "source": [
    "*Check the best hyperparams for the best ML model*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f4cd59d",
   "metadata": {},
   "source": [
    "### 9.2.1 - Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ca5e074",
   "metadata": {},
   "source": [
    "### 9.2.2 - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba96cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1099906e",
   "metadata": {},
   "source": [
    "### 9.2.3 - Bayesian Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc85ea29",
   "metadata": {},
   "source": [
    "## 9.3 - Define best hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56530fc0",
   "metadata": {},
   "source": [
    "*Explicitly define best hyper parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "118a044d",
   "metadata": {},
   "source": [
    "# **10 - PERFORMANCE EVALUATION AND INTERPRETATION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9beb0f5e",
   "metadata": {},
   "source": [
    "## 10.1 - Restore Point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06063ebc",
   "metadata": {},
   "source": [
    "*Create a checkpoint of the last dataframe from previous section*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca048da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a restore point for the previous section dataframe\n",
    "df_perform = df_tune.copy()\n",
    "\n",
    "# check dataframe\n",
    "check_dataframe( df_perform )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd259a95",
   "metadata": {},
   "source": [
    "## 10.2 - Training Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61597c50",
   "metadata": {},
   "source": [
    "*Get final model performance on training data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de669b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad7b1738",
   "metadata": {},
   "source": [
    "## 10.3 - Generalization performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86cbc993",
   "metadata": {},
   "source": [
    "### 10.3.1 - Final model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e68bfaed",
   "metadata": {},
   "source": [
    "*Get final model performance on validation data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f346c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56ad9401",
   "metadata": {},
   "source": [
    "### 10.3.2 - Error analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30f53834",
   "metadata": {},
   "source": [
    "*Perform error analysis on final model to make sure it is ready for production*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08faa83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "227e866b",
   "metadata": {},
   "source": [
    "## 10.4 - Define prodution model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3babf23",
   "metadata": {},
   "source": [
    "*Train ML on \"training + validation\" data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b33acde",
   "metadata": {},
   "source": [
    "## 10.5 - Testing performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb1220a9",
   "metadata": {},
   "source": [
    "*Get production model performance on testing data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d2e70d7",
   "metadata": {},
   "source": [
    "## 10.6 - Business performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a294eeac",
   "metadata": {},
   "source": [
    "*Translate testing performance into business results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f80bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "855aae5f",
   "metadata": {},
   "source": [
    "# **11 - DEPLOYMENT**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee8e3824",
   "metadata": {},
   "source": [
    "![Deployment architecture](../img/....jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8ebea41",
   "metadata": {},
   "source": [
    "## 11.1 - API creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33ea6360",
   "metadata": {},
   "source": [
    "*Code to create API for ML predictions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1352323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60bbaa2e",
   "metadata": {},
   "source": [
    "## 11.2 - Docker container"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e1b9c1e",
   "metadata": {},
   "source": [
    "*Code to create a Docker container and deploy ML model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bb3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0333fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hotmart_case",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
